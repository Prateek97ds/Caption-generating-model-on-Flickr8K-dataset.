# Caption-generating-model-on-Flickr8K-dataset.

Project Description: The objective of the project is to develop a deep learning model capable of providing spoken descriptions of image content by generating captions, employing an attention mechanism. This model is specifically designed to assist visually impaired individuals in comprehending images through speech. The generated captions, produced by a CNN-RNN model, will be converted into speech using a text-to-speech library.

This undertaking combines elements of deep learning and natural language processing. The image's features will be extracted by a CNN-based encoder, and subsequently, an RNN model will decode these features to create descriptive captions.

This project serves as an extended application of the "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention" paper.

The dataset used in this project is sourced from Kaggle, comprising sentence-based descriptions for 8,000 images, each associated with five distinct captions. These captions offer detailed depictions of the image's prominent elements and events.
